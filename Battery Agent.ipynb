{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df20110",
   "metadata": {},
   "source": [
    "#  MDP Value Iteration: Battery Agent Example\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This module demonstrates the Value Iteration algorithm for solving a Markov Decision Process (MDP) representing a battery management system.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f06b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dafb013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BatteryMDP:\n",
    "    \"\"\"\n",
    "    A Markov Decision Process model for battery charge management.\n",
    "    \n",
    "    States:\n",
    "        0: High Charge\n",
    "        1: Low Charge\n",
    "    \n",
    "    Actions:\n",
    "        High Charge: [0: Wait, 1: Search]\n",
    "        Low Charge: [0: Wait, 1: Search, 2: Recharge]\n",
    "    \"\"\"\n",
    "    \n",
    "    # State constants\n",
    "    HIGH_CHARGE = 0\n",
    "    LOW_CHARGE = 1\n",
    "    NUM_STATES = 2\n",
    "    \n",
    "    # Action constants\n",
    "    WAIT = 0\n",
    "    SEARCH = 1\n",
    "    RECHARGE = 2\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        gamma: float = 0.9,\n",
    "        r_wait: float = 2.0,\n",
    "        r_search: float = 3.0,\n",
    "        alpha: float = 0.7,\n",
    "        beta: float = 0.4,\n",
    "        epsilon: float = 1e-6\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Battery MDP.\n",
    "        \n",
    "        Args:\n",
    "            gamma: Discount factor (0 < gamma < 1)\n",
    "            r_wait: Reward for waiting\n",
    "            r_search: Reward for searching\n",
    "            alpha: Success probability for search in high charge\n",
    "            beta: Success probability for search in low charge\n",
    "            epsilon: Convergence threshold\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.r_wait = r_wait\n",
    "        self.r_search = r_search\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Transition probabilities\n",
    "        self.p_high_wait = 0.4\n",
    "        self.p_high_search = 0.6\n",
    "        self.p_low_wait = 0.3\n",
    "        self.p_low_search = 0.5\n",
    "        self.p_low_recharge = 0.2\n",
    "        \n",
    "        # Number of actions per state\n",
    "        self.num_actions = [2, 3]  # [High Charge, Low Charge]\n",
    "        \n",
    "        # Initialize state values\n",
    "        self.state_values = np.zeros(self.NUM_STATES)\n",
    "    \n",
    "    def compute_expected_value(self, state: int, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the expected value for a state-action pair.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (0: High, 1: Low)\n",
    "            action: Action to take\n",
    "            \n",
    "        Returns:\n",
    "            Expected value of taking the action in the given state\n",
    "        \"\"\"\n",
    "        if state == self.HIGH_CHARGE:\n",
    "            return self._compute_high_charge_value(action)\n",
    "        else:\n",
    "            return self._compute_low_charge_value(action)\n",
    "    \n",
    "    def _compute_high_charge_value(self, action: int) -> float:\n",
    "        \"\"\"Compute expected value for high charge state.\"\"\"\n",
    "        if action == self.WAIT:\n",
    "            # Wait action: stay in high charge with probability p_high_wait\n",
    "            return self.p_high_wait * (\n",
    "                self.r_wait + self.gamma * self.state_values[self.HIGH_CHARGE]\n",
    "            )\n",
    "        elif action == self.SEARCH:\n",
    "            # Search action: stay high with prob alpha, go low with prob (1-alpha)\n",
    "            stay_high = self.alpha * (\n",
    "                self.r_search + self.gamma * self.state_values[self.HIGH_CHARGE]\n",
    "            )\n",
    "            go_low = (1 - self.alpha) * (\n",
    "                self.r_search + self.gamma * self.state_values[self.LOW_CHARGE]\n",
    "            )\n",
    "            return self.p_high_search * (stay_high + go_low)\n",
    "        return 0.0\n",
    "    \n",
    "    def _compute_low_charge_value(self, action: int) -> float:\n",
    "        \"\"\"Compute expected value for low charge state.\"\"\"\n",
    "        if action == self.WAIT:\n",
    "            # Wait action: stay in low charge\n",
    "            return self.p_low_wait * (\n",
    "                self.r_wait + self.gamma * self.state_values[self.LOW_CHARGE]\n",
    "            )\n",
    "        elif action == self.SEARCH:\n",
    "            # Search action: stay low with prob beta, go high with prob (1-beta)\n",
    "            # Note: penalty of -3 when transitioning to high (risky search)\n",
    "            stay_low = self.beta * (\n",
    "                self.r_search + self.gamma * self.state_values[self.LOW_CHARGE]\n",
    "            )\n",
    "            go_high = (1 - self.beta) * (\n",
    "                -3 + self.gamma * self.state_values[self.HIGH_CHARGE]\n",
    "            )\n",
    "            return self.p_low_search * (stay_low + go_high)\n",
    "        elif action == self.RECHARGE:\n",
    "            # Recharge action: transition to high charge\n",
    "            return self.p_low_recharge * (\n",
    "                self.gamma * self.state_values[self.HIGH_CHARGE]\n",
    "            )\n",
    "        return 0.0\n",
    "    \n",
    "    def value_iteration(self, verbose: bool = True) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"\n",
    "        Perform value iteration to find optimal state values.\n",
    "        \n",
    "        Args:\n",
    "            verbose: Whether to print iteration progress\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (optimal state values, number of iterations)\n",
    "        \"\"\"\n",
    "        iteration = 0\n",
    "        \n",
    "        while True:\n",
    "            new_values = np.copy(self.state_values)\n",
    "            delta = 0.0\n",
    "            iteration += 1\n",
    "            \n",
    "            # Update each state\n",
    "            for state in range(self.NUM_STATES):\n",
    "                # Compute expected value for each action\n",
    "                action_values = [\n",
    "                    self.compute_expected_value(state, action)\n",
    "                    for action in range(self.num_actions[state])\n",
    "                ]\n",
    "                \n",
    "                # Bellman optimality: take max over actions\n",
    "                new_values[state] = max(action_values)\n",
    "                \n",
    "                # Track maximum change\n",
    "                delta = max(delta, abs(self.state_values[state] - new_values[state]))\n",
    "            \n",
    "            self.state_values = new_values\n",
    "            \n",
    "            if verbose and iteration % 10 == 0:\n",
    "                print(f\"Iteration {iteration}: δ = {delta:.6f}\")\n",
    "            \n",
    "            # Check convergence\n",
    "            if delta < self.epsilon:\n",
    "                break\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n✓ Converged in {iteration} iterations (δ = {delta:.6e})\")\n",
    "        \n",
    "        return self.state_values, iteration\n",
    "    \n",
    "    def extract_policy(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Extract the optimal policy from converged state values.\n",
    "        \n",
    "        Returns:\n",
    "            List of optimal actions for each state\n",
    "        \"\"\"\n",
    "        policy = []\n",
    "        \n",
    "        for state in range(self.NUM_STATES):\n",
    "            action_values = [\n",
    "                self.compute_expected_value(state, action)\n",
    "                for action in range(self.num_actions[state])\n",
    "            ]\n",
    "            optimal_action = int(np.argmax(action_values))\n",
    "            policy.append(optimal_action)\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"Print the optimal state values and policy.\"\"\"\n",
    "        state_names = [\"High Charge\", \"Low Charge\"]\n",
    "        action_names = {\n",
    "            self.WAIT: \"Wait\",\n",
    "            self.SEARCH: \"Search\",\n",
    "            self.RECHARGE: \"Recharge\"\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"OPTIMAL STATE VALUES\")\n",
    "        print(\"=\"*60)\n",
    "        for state in range(self.NUM_STATES):\n",
    "            print(f\"{state_names[state]:15s} V*(s) = {self.state_values[state]:7.4f}\")\n",
    "        \n",
    "        policy = self.extract_policy()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"OPTIMAL POLICY\")\n",
    "        print(\"=\"*60)\n",
    "        for state in range(self.NUM_STATES):\n",
    "            action_name = action_names[policy[state]]\n",
    "            print(f\"{state_names[state]:15s} π*(s) = {action_name}\")\n",
    "        print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4f0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"MDP Value Iteration: Battery Agent Example\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create and solve the MDP\n",
    "    mdp = BatteryMDP(\n",
    "        gamma=0.9,\n",
    "        r_wait=2.0,\n",
    "        r_search=3.0,\n",
    "        alpha=0.7,\n",
    "        beta=0.4,\n",
    "        epsilon=1e-6\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nInitial State Values: {mdp.state_values}\")\n",
    "    print(\"\\nRunning Value Iteration...\")\n",
    "    \n",
    "    # Perform value iteration\n",
    "    optimal_values, num_iterations = mdp.value_iteration(verbose=True)\n",
    "    \n",
    "    # Display results\n",
    "    mdp.print_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bb224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Value Iteration: Battery Agent Example\n",
      "============================================================\n",
      "\n",
      "Initial State Values: [0. 0.]\n",
      "\n",
      "Running Value Iteration...\n",
      "Iteration 10: δ = 0.000419\n",
      "\n",
      "✓ Converged in 17 iterations (δ = 4.683466e-07)\n",
      "\n",
      "============================================================\n",
      "OPTIMAL STATE VALUES\n",
      "============================================================\n",
      "High Charge     V*(s) =  3.1080\n",
      "Low Charge      V*(s) =  0.8219\n",
      "\n",
      "============================================================\n",
      "OPTIMAL POLICY\n",
      "============================================================\n",
      "High Charge     π*(s) = Search\n",
      "Low Charge      π*(s) = Wait\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03726de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
